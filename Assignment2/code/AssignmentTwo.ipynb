{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "(1) [Mix of Gaussian](https://pythonmachinelearning.pro/clustering-with-gaussian-mixture-models/).\n",
    "\n",
    "(2) [GMM](https://towardsdatascience.com/how-to-code-gaussian-mixture-models-from-scratch-in-python-9e7975df5252).\n",
    "\n",
    "(3) [Python Abstracted Classes](https://www.python-course.eu/python3_abstract_classes.php)\n",
    "\n",
    "(4) [Gaussian mixture and EM algorithim-Recommended](https://jonathan-hui.medium.com/machine-learning-expectation-maximization-algorithm-em-2e954cb76959)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from abc import ABC, abstractmethod \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data fetcher \n",
    "class DataFetcher:\n",
    "    def __init__(self, directory, X_name, Y_name, SUBSETS):\n",
    "        self.directory = directory\n",
    "        self.X_name = X_name\n",
    "        self.Y_name = Y_name\n",
    "        self.SUBSETS = SUBSETS\n",
    "    \n",
    "    # get training X path\n",
    "    # return path of the training data\n",
    "\n",
    "    def _getTrainingXPath(self):\n",
    "        return \"./%s/train%s.csv\" % (self.directory, self.X_name)\n",
    "\n",
    "    def _getTrainingYPath(self):\n",
    "        return \"./%s/train%s.csv\" % (self.directory, self.Y_name)\n",
    "\n",
    "    def _getTrainingXPathSubsets(self, subset_num):\n",
    "        # 0 < subset <= 9\n",
    "        return \"./%s/train%s%d.csv\" % (self.directory, self.X_name, (subset_num+1))\n",
    "\n",
    "    # get training Y path\n",
    "    # return path of traning labels\n",
    "    def _getTrainingYPathSubsets(self, subset_num):\n",
    "        # 0 < subset <= 9\n",
    "        return \"./%s/train%s%d.csv\" % (self.directory, self.Y_name, (subset_num+1))\n",
    "\n",
    "    # get all traning X Y \n",
    "    def getAllTrainingXY(self):\n",
    "        if self.SUBSETS:\n",
    "            trainingX_dfs = []\n",
    "            trainingY_dfs = []\n",
    "            for i in range(NUM_SUBSETS):\n",
    "                trainingX_dfs.append(pd.read_csv(self._getTrainingXPathSubsets(i), header=None))\n",
    "                trainingY_dfs.append(pd.read_csv(self._getTrainingYPathSubsets(i), header=None))\n",
    "            return trainingX_dfs, trainingY_dfs\n",
    "        else:\n",
    "            trainingX_dfs = pd.read_csv(self._getTrainingXPath(), header=None)\n",
    "            trainingY_dfs = pd.read_csv(self._getTrainingYPath(), header=None)\n",
    "\n",
    "\n",
    "    # #get test X Y\n",
    "    def getAllTestXY(self):\n",
    "        testX_path = \"./%s/test%s.csv\" % (self.directory, self.X_name)\n",
    "        testY_path = \"./%s/test%s.csv\" % (self.directory, self.Y_name)\n",
    "        testX_df = pd.read_csv(testX_path, header=None)\n",
    "        testY_df = pd.read_csv(testY_path, header=None)\n",
    "        return testX_df, testY_df\n",
    "\n",
    "#data = DataFetcher('Knn-dataset', 'Data', 'Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "class CrossValidation:\n",
    "    def __init__(self, x_dfs, y_dfs):\n",
    "        self.X = x_dfs\n",
    "        self.Y = y_dfs\n",
    "        self.subsets_num = len(x_dfs)\n",
    "\n",
    "        #split training validation\n",
    "        #pick one subset for validation and the rest for training(used for both X, and Y)\n",
    "    def _splitTrainingValidation(self, dfs, subset_num):\n",
    "\n",
    "        validation_df = dfs[subset_num]\n",
    "\n",
    "        training_df = dfs[:subset_num] + dfs[subset_num+1:]\n",
    "        training_df = pd.concat(training_df, ignore_index=True)\n",
    "\n",
    "        return training_df, validation_df\n",
    "\n",
    "\n",
    "        #get training validation X\n",
    "    def _getTrainingValidationX(self, subset_num):\n",
    "        training_X, validation_X = self._splitTrainingValidation(self.X, subset_num)\n",
    "        return training_X, validation_X\n",
    "\n",
    "        #get training validation Y\n",
    "    def _getTrainingValidationY(self, subset_num):\n",
    "        training_Y, validation_Y = self._splitTrainingValidation(self.Y, subset_num)\n",
    "        return training_Y, validation_Y\n",
    "\n",
    "        #get training validation XY \n",
    "    def getTrainingValidationXY(self, subset_num):\n",
    "        training_X, validation_X = self._getTrainingValidationX(subset_num)\n",
    "        training_Y, validation_Y = self._getTrainingValidationY(subset_num)\n",
    "\n",
    "        return [training_X, training_Y, validation_X, validation_Y]\n",
    "            \n",
    "\n",
    "        #get all X Y\n",
    "    def getAllXY(self):\n",
    "        training_X = pd.concat(self.X, ignore_index=True)\n",
    "        training_Y = pd.concat(self.Y, ignore_index=True)\n",
    "\n",
    "        return [training_X, training_Y]\n",
    "\n",
    "def getAccuracy(actual_y, predicted_y):\n",
    "    assert(actual_y.__len__()==predicted_y.__len__())\n",
    "    accuracy = sum(1 for y, y_hat in zip(actual_y, predicted_y) if y == y_hat)/len(actual_y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model class\n",
    "class Model(ABC):\n",
    "    def __init__(self, train_x, train_y):\n",
    "        self.X = train_x\n",
    "        self.Y = train_y\n",
    "    \n",
    "    @classmethod\n",
    "    def predict(self, x):\n",
    "        pass\n",
    "\n",
    "    def predict_df(self, X_df):\n",
    "        predictions = X_df.apply(lambda row: self.predict(row), raw=True, axis=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data \n",
    "DIR_NAME = 'knn-dataset'\n",
    "X_NAME = 'Data'\n",
    "Y_NAME = 'Labels'\n",
    "NUM_SUBSETS = 10\n",
    "POSITIVE_LABEL = 6\n",
    "SUBSETS = True\n",
    "NEGATIVE_LABEL = 5\n",
    "LAMBDAS = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  54  55  56  57  58  59  60  \\\n",
       "0   0   0   0   1  13  10   0   0   0   8  ...  16   0   0   0   4   0  14   \n",
       "1   0   0   0   0   0   7   0   0   0   0  ...  16   0   0   0   0   0   2   \n",
       "2  16  11   5  16  15  16  16  16  16  16  ...  14  16  16  16  16  16  16   \n",
       "3   0   0   7  16   2   2   0   0   0   1  ...  11   0   0   0   0   9   9   \n",
       "4   0   0   0   7   0   0   0   0   0   0  ...   0   0   0   0   0  13  14   \n",
       "\n",
       "   61  62  63  \n",
       "0   0  15   0  \n",
       "1  11  12   0  \n",
       "2  16  16   0  \n",
       "3   0   0   0  \n",
       "4  12   0   0  \n",
       "\n",
       "[5 rows x 64 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>54</th>\n      <th>55</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n      <th>60</th>\n      <th>61</th>\n      <th>62</th>\n      <th>63</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>...</td>\n      <td>16</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>14</td>\n      <td>0</td>\n      <td>15</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>16</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>11</td>\n      <td>12</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16</td>\n      <td>11</td>\n      <td>5</td>\n      <td>16</td>\n      <td>15</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>...</td>\n      <td>14</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>16</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13</td>\n      <td>14</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 64 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "#test data\n",
    "DF = DataFetcher(DIR_NAME, X_NAME, Y_NAME, SUBSETS)\n",
    "test_X, test_Y = DF.getAllTestXY()\n",
    "test_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   0\n",
       "0  6\n",
       "1  6\n",
       "2  5\n",
       "3  6\n",
       "4  6"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "test_Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training X shape:(900, 64)\n"
     ]
    }
   ],
   "source": [
    "#training validation split\n",
    "training_X_dfs, training_Y_dfs = DF.getAllTrainingXY()\n",
    "CVData = CrossValidation(training_X_dfs, training_Y_dfs)\n",
    "print(\"Training X shape:\"+ str(CVData.getTrainingValidationXY(0)[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tarining Y shape:(900, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Tarining Y shape:\"+ str(CVData.getTrainingValidationXY(0)[1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation X shape:(100, 64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation X shape:\"+ str(CVData.getTrainingValidationXY(0)[2].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation Y shape:(100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Y shape:\"+ str(CVData.getTrainingValidationXY(0)[3].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixture of Gaussian \n",
    "class MixtureOfGaussian(Model):\n",
    "    def __init__(self, train_x, train_y, positive_label, negative_label):\n",
    "        super().__init__(train_x, train_y)\n",
    "        self.positive_label = positive_label\n",
    "        self.negative_label = negative_label\n",
    "        \n",
    "    \n",
    "        # MLE Estimation for parameters\n",
    "        \n",
    "        y_list = train_y.values.flatten().tolist() #convert df to seq list\n",
    "\n",
    "        pi = sum([1 if y == self.positive_label else 0 for y in y_list])/len(y_list)   # pi = summation-n(yn/N)\n",
    "\n",
    "\n",
    "        #calculate mu1, mu2, s1, s2, cov, inv_cov\n",
    "        \n",
    "        positive_indices = [ind for ind, y in enumerate(y_list) if y == positive_label]\n",
    "        negative_indices = [ind for ind, y in enumerate(y_list) if y != positive_label] \n",
    "\n",
    "        n1 = len(positive_indices)\n",
    "        n2 = len(negative_indices)\n",
    "\n",
    "        N = n1 + n2\n",
    "\n",
    "        positive_X_df = train_x.iloc[positive_indices]\n",
    "        negative_X_df = train_x.iloc[negative_indices]\n",
    "\n",
    "        mu1 = positive_X_df.mean(axis=0)\n",
    "        mu2 = negative_X_df.mean(axis=0)\n",
    "\n",
    "        positive_X = positive_X_df.to_numpy()\n",
    "        negative_X = negative_X_df.to_numpy()\n",
    "\n",
    "        positive_dists = positive_X_df.to_numpy() - mu1.to_numpy()\n",
    "        negative_dists = negative_X_df.to_numpy() - mu2.to_numpy()\n",
    "\n",
    "        s1 = positive_dists.T.dot(positive_dists) / n1\n",
    "        s2 = positive_dists.T.dot(positive_dists) / n2\n",
    "\n",
    "        cov = (n1/N) * s1 + (n2/N) * s2\n",
    "        cov_inv = np.linalg.inv(cov)\n",
    "\n",
    "        w = cov_inv.dot(mu1 - mu2)\n",
    "        w_0 = -(1/2) * mu1.T.dot(cov_inv).dot(mu1) + (1/2) * mu2.T.dot(cov_inv).dot(mu2) + np.log(pi/ (1-pi))\n",
    "\n",
    "        self.pi = pi\n",
    "        self.n1 = n1\n",
    "        self.n2 = n2\n",
    "        self.mu1 = mu1\n",
    "        self.mu2 = mu2\n",
    "        self.cov = cov\n",
    "        self.w = w\n",
    "        self.w_0 = w_0 \n",
    "\n",
    "    def predict_prob(self, x):\n",
    "        logit_odds = self.w.dot(x) + self.w_0\n",
    "        prob = 1 / (1 + np.exp(-logit_odds))        \n",
    "        return prob\n",
    "    \n",
    "    def predict(self, x):\n",
    "        prob = self.predict_prob(x)\n",
    "        if prob > 0.5:\n",
    "            return self.positive_label\n",
    "        else:\n",
    "            return self.negative_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Accuracy: 0.836000\nTest Accuracy: 0.854545\nTraining Time: 0.022997617721557617\n"
     ]
    }
   ],
   "source": [
    "#train and test \n",
    "train_x, train_y = CVData.getAllXY()\n",
    "\n",
    "# Create hypothesis\n",
    "t1 = time.time()\n",
    "model = MixtureOfGaussian(train_x, train_y, POSITIVE_LABEL, NEGATIVE_LABEL)\n",
    "t2 = time.time()\n",
    "\n",
    "# Train Accuracy \n",
    "predicted_train_y = model.predict_df(train_x)\n",
    "train_accuracy = getAccuracy(train_y.values.flatten(), predicted_train_y.values.flatten())\n",
    "\n",
    "# Test Accuracy\n",
    "predicted_test_y = model.predict_df(test_X)\n",
    "test_accuracy = getAccuracy(test_Y.values.flatten(), predicted_test_y.values.flatten())\n",
    "\n",
    "\n",
    "print(\"Train Accuracy: %f\" % train_accuracy)\n",
    "print(\"Test Accuracy: %f\" % test_accuracy)\n",
    "print(\"Training Time: \" + str(t2-t1))"
   ]
  },
  {
   "source": [
    "## Logistic Regression Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(Model):\n",
    "    def __init__(self, train_X, train_y, positive_label, negative_label, lmbda=0,  max_iters=10, threshold=0.01, train_intercept=True):\n",
    "        super().__init__(train_X, train_y)\n",
    "        self.positive_label = positive_label\n",
    "        self.negative_label = negative_label\n",
    "        self.lmbda = lmbda\n",
    "        self.max_iters = max_iters\n",
    "        self.threshold = threshold\n",
    "        self.train_intercept = train_intercep\n",
    "\n",
    "        X = train_X.to_numpy()\n",
    "        y = train_y.to_numpy()\n",
    "\n",
    "        # y's as 1 & 0's\n",
    "        y = [1 if y_i == positive_label else 0 for y_i in y]\n",
    "\n",
    "        # Add column of ones\n",
    "        if train_intercept:\n",
    "            X = np.insert(X, 0, np.ones(len(X)), axis=1)\n",
    "\n",
    "        self.X = X                                  #(1000, 64) Shape\n",
    "        self.y = y                                  #(1000,1) Shape\n",
    "\n",
    "        self.w = np.zeros(X.shape[1])               #(64,) Shape\n",
    "        self._train(self.X, self.w, self.y)        \n",
    "\n",
    "        def _sigmoid(self, odds):\n",
    "            return 1 / 1 + np.exp(-odds)                \n",
    "        \n",
    "        def _gradient(self, X, w, y):\n",
    "            weighted_values = X.dot(w)\n",
    "            # probabiliteis = np.apply_along_axis(self._sigmoid, 0, weighted_values)\n",
    "            probabiliteis = self._sigmoid(weighted_values)#faster 4X than using apply_along_axis\n",
    "            return X.T.dot(probabiliteis-y) + 0.5 * self.lmbda * w.T.dot(w)\n",
    "\n",
    "        def _invProbability(self, probability):\n",
    "            return 1-probability\n",
    "\n",
    "        def _R(self, X, w):\n",
    "            weighted_values = X.dot(w)\n",
    "            probabilities = self._sigmoid(weighted_values)\n",
    "            inv_probabilities = self._invProbability(weighted_values)\n",
    "            return np.diag(np.multiply(probabiliteis, inv_probabilities))\n",
    "\n",
    "        def _Hassian(self, X, w):\n",
    "            R = self._R(X, w)\n",
    "            H = X.T.dot(R).dot(X)\n",
    "            return H + self.lmbda * np.identity(len(H))\n",
    "        \n",
    "        def _train(self, X, w, y):\n",
    "            iters = 0 \n",
    "            while (True):\n",
    "                if iters >= self.max_iters:\n",
    "                    break\n",
    "\n",
    "                gradient = self._gradient(X, w, y)\n",
    "                hassian = self._Hassian(X, w)\n",
    "                hassian_inv = np.linalg.inv(hassian)\n",
    "\n",
    "                w_new = w - hassian_inv.dot(gradient)\n",
    "\n",
    "                dist = norm(w_new - w)\n",
    "\n",
    "                w = w_new\n",
    "\n",
    "                if dist < self.threshold:\n",
    "                    break\n",
    "\n",
    "                iters += 1\n",
    "            self.w = w \n",
    "        \n",
    "        def predict_prob(self, x):\n",
    "            if self.train_intercept:\n",
    "                x = np.insert(x, 0, 1)\n",
    "            odds = self.w.dot(x) \n",
    "            return self._sigmoid(odds)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        prob = self.predict_prob(x)\n",
    "        if prob > 0.5:\n",
    "            return self.positive_label\n",
    "        else:\n",
    "            return self.negative_label\n",
    "\n",
    "        "
   ]
  },
  {
   "source": [
    "## Cross Validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_logreg_CV():\n",
    "\n",
    "    average_accuracies = []\n",
    "    for lmbda in tqdm_notebook(LAMBDAS):\n",
    "        # Perform CV\n",
    "        accuracies = []\n",
    "        for j in range(NUM_SUBSETS):\n",
    "            train_X, train_y, validation_X, validation_y = CVData.get_training_validation_X_y(j)\n",
    "            \n",
    "            # Create hypothesis\n",
    "            model = LogisticRegression(train_X, train_y, POSITIVE_LABEL, NEGATIVE_LABEL, lmbda)\n",
    "            predicted_validation_y = model.predict_df(validation_X)\n",
    "\n",
    "            # Get accuracy\n",
    "            accuracy = get_accuracy(validation_y.values.flatten(), predicted_validation_y.values.flatten())\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "        avg_accuracy = np.mean(accuracies)\n",
    "        average_accuracies.append(avg_accuracy)\n",
    "\n",
    "    return average_accuracies\n"
   ]
  },
  {
   "source": [
    "## Finding Optimal Lambda "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\User\\miniconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-183-29e0e2de41a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcv_accuracies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperform_logreg_CV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moptimal_lambda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLAMBDAS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_accuracies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The optimal lambda is %f with an average accuracy of %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moptimal_lambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_accuracies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-182-91d22a7ecc54>\u001b[0m in \u001b[0;36mperform_logreg_CV\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0maverage_accuracies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mlmbda\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLAMBDAS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;31m# Perform CV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0maccuracies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tf2\\lib\\site-packages\\tqdm\\__init__.py\u001b[0m in \u001b[0;36mtqdm_notebook\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m          \u001b[1;34m\"Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m          TqdmDeprecationWarning, stacklevel=2)\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_tqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tf2\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[0munit_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_printer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mncols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdisplay_here\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tf2\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mIProgress\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# #187 #451 #558 #872\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             raise ImportError(\n\u001b[1;32m--> 118\u001b[1;33m                 \u001b[1;34m\"IProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m                 \u001b[1;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \"/user_install.html\")\n",
      "\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "cv_accuracies = perform_logreg_CV()\n",
    "optimal_lambda = LAMBDAS[np.argsort(cv_accuracies)[-1]]\n",
    "print(\"The optimal lambda is %f with an average accuracy of %f\" % (optimal_lambda, max(cv_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting ipywidgets\n  Downloading ipywidgets-7.6.3-py2.py3-none-any.whl (121 kB)\nRequirement already satisfied: nbformat>=4.2.0 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from ipywidgets) (5.0.7)\nRequirement already satisfied: traitlets>=4.3.1 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from ipywidgets) (4.3.3)\nRequirement already satisfied: ipykernel>=4.5.1 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from ipywidgets) (5.3.4)\nRequirement already satisfied: ipython>=4.0.0 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from ipywidgets) (7.16.1)\nRequirement already satisfied: jupyter-client in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.7)\nRequirement already satisfied: tornado>=4.2 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\nRequirement already satisfied: jedi>=0.10 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.17.2)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (3.0.7)\nRequirement already satisfied: pickleshare in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\nRequirement already satisfied: decorator in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\nRequirement already satisfied: backcall in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\nRequirement already satisfied: pygments in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (2.7.1)\nRequirement already satisfied: colorama in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.4.4)\nRequirement already satisfied: setuptools>=18.5 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (49.6.0.post20201009)\nRequirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets) (0.7.1)\nCollecting jupyterlab-widgets>=1.0.0\n  Downloading jupyterlab_widgets-1.0.0-py3-none-any.whl (243 kB)\nRequirement already satisfied: jupyter-core in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (4.6.3)\nRequirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\nRequirement already satisfied: ipython-genutils in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\nRequirement already satisfied: importlib-metadata in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (2.0.0)\nRequirement already satisfied: six>=1.11.0 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.15.0)\nRequirement already satisfied: attrs>=17.4.0 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (20.2.0)\nRequirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.17.3)\nRequirement already satisfied: wcwidth in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\nCollecting widgetsnbextension~=3.5.0\n  Downloading widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\nRequirement already satisfied: notebook>=4.4.1 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.1.4)\nRequirement already satisfied: argon2-cffi in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.1.0)\nRequirement already satisfied: nbconvert in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.0.6)\nRequirement already satisfied: terminado>=0.8.3 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.9.1)\nRequirement already satisfied: prometheus-client in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.0)\nRequirement already satisfied: pyzmq>=17 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (19.0.2)\nRequirement already satisfied: Send2Trash in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\nRequirement already satisfied: jinja2 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.2)\nRequirement already satisfied: python-dateutil>=2.1 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.1)\nRequirement already satisfied: pywin32>=1.0 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->ipywidgets) (228)\nRequirement already satisfied: pywinpty>=0.5 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.7)\nRequirement already satisfied: cffi>=1.0.0 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.3)\nRequirement already satisfied: pycparser in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.20)\nRequirement already satisfied: zipp>=0.5 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.2.0)\nRequirement already satisfied: MarkupSafe>=0.23 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.1.1)\nRequirement already satisfied: entrypoints>=0.2.2 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\nRequirement already satisfied: bleach in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.2.1)\nRequirement already satisfied: jupyterlab-pygments in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\nRequirement already satisfied: testpath in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.4)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.2)\nRequirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\nRequirement already satisfied: defusedxml in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.6.0)\nRequirement already satisfied: async-generator in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.10)\nRequirement already satisfied: nest-asyncio in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.1)\nRequirement already satisfied: packaging in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.4)\nRequirement already satisfied: webencodings in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\nRequirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\miniconda3\\envs\\tf2\\lib\\site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\nInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\nSuccessfully installed ipywidgets-7.6.3 jupyterlab-widgets-1.0.0 widgetsnbextension-3.5.1\nNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_x = train_x.to_numpy()\n",
    "weights_random = np.random.rand(training_data_x.shape[1])\n",
    "def sigmoid(odds):\n",
    "            return 1 / 1 + np.exp(-odds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "metadata": {},
     "execution_count": 171
    }
   ],
   "source": [
    "weighted_values = training_data_x.dot(weights_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8.01 µs ± 70.2 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sigmoid(weighted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "metadata": {},
     "execution_count": 175
    }
   ],
   "source": [
    "np.diag(np.multiply(sigmoid(weighted_values), sigmoid(weighted_values))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "47.3 µs ± 828 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.apply_along_axis(sigmoid, 0, weighted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, w, y):\n",
    "    weighted_values = x.dot(w)\n",
    "    square_part = sigmoid(weighted_values) - y \n",
    "    return square_part.T.dot(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5.19 ms ± 242 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit gradients = gradient(training_data_x, weights_random, train_y.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 64)"
      ]
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "source": [
    "gradients = gradient(training_data_x, weights_random, train_y.to_numpy())\n",
    "gradients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_along_axis(x, w, y):\n",
    "    weighted_values = x.dot(w)\n",
    "    probabilities = np.apply_along_axis(sigmoid, 0, weighted_values)\n",
    "    return (probabilities-y).T.dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7.12 ms ± 173 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit gradient_along_axis(training_data_x, weights_random, train_y.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 64)"
      ]
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "gradients_2 = gradient_along_axis(training_data_x, weights_random, train_y.to_numpy())\n",
    "gradients_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}