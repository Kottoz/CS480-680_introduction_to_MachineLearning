{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "(1) [Mix of Gaussian](https://pythonmachinelearning.pro/clustering-with-gaussian-mixture-models/).\n",
    "\n",
    "(2) [GMM](https://towardsdatascience.com/how-to-code-gaussian-mixture-models-from-scratch-in-python-9e7975df5252).\n",
    "\n",
    "(3) [Python Abstracted Classes](https://www.python-course.eu/python3_abstract_classes.php)\n",
    "\n",
    "(4) [Gaussian mixture and EM algorithim-Recommended](https://jonathan-hui.medium.com/machine-learning-expectation-maximization-algorithm-em-2e954cb76959)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from abc import ABC, abstractmethod \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data fetcher \n",
    "class DataFetcher:\n",
    "    def __init__(self, directory, X_name, Y_name, SUBSETS):\n",
    "        self.directory = directory\n",
    "        self.X_name = X_name\n",
    "        self.Y_name = Y_name\n",
    "        self.SUBSETS = SUBSETS\n",
    "    \n",
    "    # get training X path\n",
    "    # return path of the training data\n",
    "\n",
    "    def _getTrainingXPath(self):\n",
    "        return \"./%s/train%s.csv\" % (self.directory, self.X_name)\n",
    "\n",
    "    def _getTrainingYPath(self):\n",
    "        return \"./%s/train%s.csv\" % (self.directory, self.Y_name)\n",
    "\n",
    "    def _getTrainingXPathSubsets(self, subset_num):\n",
    "        # 0 < subset <= 9\n",
    "        return \"./%s/train%s%d.csv\" % (self.directory, self.X_name, (subset_num+1))\n",
    "\n",
    "    # get training Y path\n",
    "    # return path of traning labels\n",
    "    def _getTrainingYPathSubsets(self, subset_num):\n",
    "        # 0 < subset <= 9\n",
    "        return \"./%s/train%s%d.csv\" % (self.directory, self.Y_name, (subset_num+1))\n",
    "\n",
    "    # get all traning X Y \n",
    "    def getAllTrainingXY(self):\n",
    "        if self.SUBSETS:\n",
    "            trainingX_dfs = []\n",
    "            trainingY_dfs = []\n",
    "            for i in range(NUM_SUBSETS):\n",
    "                trainingX_dfs.append(pd.read_csv(self._getTrainingXPathSubsets(i), header=None))\n",
    "                trainingY_dfs.append(pd.read_csv(self._getTrainingYPathSubsets(i), header=None))\n",
    "            return trainingX_dfs, trainingY_dfs\n",
    "        else:\n",
    "            trainingX_dfs = pd.read_csv(self._getTrainingXPath(), header=None)\n",
    "            trainingY_dfs = pd.read_csv(self._getTrainingYPath(), header=None)\n",
    "\n",
    "\n",
    "    # #get test X Y\n",
    "    def getAllTestXY(self):\n",
    "        testX_path = \"./%s/test%s.csv\" % (self.directory, self.X_name)\n",
    "        testY_path = \"./%s/test%s.csv\" % (self.directory, self.Y_name)\n",
    "        testX_df = pd.read_csv(testX_path, header=None)\n",
    "        testY_df = pd.read_csv(testY_path, header=None)\n",
    "        return testX_df, testY_df\n",
    "\n",
    "#data = DataFetcher('Knn-dataset', 'Data', 'Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "class CrossValidation:\n",
    "    def __init__(self, x_dfs, y_dfs):\n",
    "        self.X = x_dfs\n",
    "        self.Y = y_dfs\n",
    "        self.subsets_num = len(x_dfs)\n",
    "\n",
    "        #split training validation\n",
    "        #pick one subset for validation and the rest for training(used for both X, and Y)\n",
    "    def _splitTrainingValidation(self, dfs, subset_num):\n",
    "\n",
    "        validation_df = dfs[subset_num]\n",
    "\n",
    "        training_df = dfs[:subset_num] + dfs[subset_num+1:]\n",
    "        training_df = pd.concat(training_df, ignore_index=True)\n",
    "\n",
    "        return training_df, validation_df\n",
    "\n",
    "\n",
    "        #get training validation X\n",
    "    def _getTrainingValidationX(self, subset_num):\n",
    "        training_X, validation_X = self._splitTrainingValidation(self.X, subset_num)\n",
    "        return training_X, validation_X\n",
    "\n",
    "        #get training validation Y\n",
    "    def _getTrainingValidationY(self, subset_num):\n",
    "        training_Y, validation_Y = self._splitTrainingValidation(self.Y, subset_num)\n",
    "        return training_Y, validation_Y\n",
    "\n",
    "        #get training validation XY \n",
    "    def getTrainingValidationXY(self, subset_num):\n",
    "        training_X, validation_X = self._getTrainingValidationX(subset_num)\n",
    "        training_Y, validation_Y = self._getTrainingValidationY(subset_num)\n",
    "\n",
    "        return [training_X, training_Y, validation_X, validation_Y]\n",
    "            \n",
    "\n",
    "        #get all X Y\n",
    "    def getAllXY(self):\n",
    "        training_X = pd.concat(self.X, ignore_index=True)\n",
    "        training_Y = pd.concat(self.Y, ignore_index=True)\n",
    "\n",
    "        return [training_X, training_Y]\n",
    "\n",
    "def getAccuracy(actual_y, predicted_y):\n",
    "    assert(actual_y.__len__()==predicted_y.__len__())\n",
    "    accuracy = sum(1 for y, y_hat in zip(actual_y, predicted_y) if y == y_hat)/len(actual_y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model class\n",
    "class Model(ABC):\n",
    "    def __init__(self, train_x, train_y):\n",
    "        self.X = train_x\n",
    "        self.Y = train_y\n",
    "    \n",
    "    @classmethod\n",
    "    def predict(self, x):\n",
    "        pass\n",
    "\n",
    "    def predict_df(self, X_df):\n",
    "        predictions = X_df.apply(lambda row: self.predict(row), raw=True, axis=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data \n",
    "DIR_NAME = 'knn-dataset'\n",
    "X_NAME = 'Data'\n",
    "Y_NAME = 'Labels'\n",
    "NUM_SUBSETS = 10\n",
    "POSITIVE_LABEL = 6\n",
    "SUBSETS = True\n",
    "NEGATIVE_LABEL = 5\n",
    "LAMBDAS = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  54  55  56  57  58  59  60  \\\n",
       "0   0   0   0   1  13  10   0   0   0   8  ...  16   0   0   0   4   0  14   \n",
       "1   0   0   0   0   0   7   0   0   0   0  ...  16   0   0   0   0   0   2   \n",
       "2  16  11   5  16  15  16  16  16  16  16  ...  14  16  16  16  16  16  16   \n",
       "3   0   0   7  16   2   2   0   0   0   1  ...  11   0   0   0   0   9   9   \n",
       "4   0   0   0   7   0   0   0   0   0   0  ...   0   0   0   0   0  13  14   \n",
       "\n",
       "   61  62  63  \n",
       "0   0  15   0  \n",
       "1  11  12   0  \n",
       "2  16  16   0  \n",
       "3   0   0   0  \n",
       "4  12   0   0  \n",
       "\n",
       "[5 rows x 64 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>54</th>\n      <th>55</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n      <th>60</th>\n      <th>61</th>\n      <th>62</th>\n      <th>63</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>...</td>\n      <td>16</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>14</td>\n      <td>0</td>\n      <td>15</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>16</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>11</td>\n      <td>12</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16</td>\n      <td>11</td>\n      <td>5</td>\n      <td>16</td>\n      <td>15</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>...</td>\n      <td>14</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>16</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13</td>\n      <td>14</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 64 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "#test data\n",
    "DF = DataFetcher(DIR_NAME, X_NAME, Y_NAME, SUBSETS)\n",
    "test_X, test_Y = DF.getAllTestXY()\n",
    "test_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   0\n",
       "0  6\n",
       "1  6\n",
       "2  5\n",
       "3  6\n",
       "4  6"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "test_Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training X shape:(900, 64)\n"
     ]
    }
   ],
   "source": [
    "#training validation split\n",
    "training_X_dfs, training_Y_dfs = DF.getAllTrainingXY()\n",
    "CVData = CrossValidation(training_X_dfs, training_Y_dfs)\n",
    "print(\"Training X shape:\"+ str(CVData.getTrainingValidationXY(0)[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tarining Y shape:(900, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Tarining Y shape:\"+ str(CVData.getTrainingValidationXY(0)[1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation X shape:(100, 64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation X shape:\"+ str(CVData.getTrainingValidationXY(0)[2].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation Y shape:(100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Y shape:\"+ str(CVData.getTrainingValidationXY(0)[3].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixture of Gaussian \n",
    "class MixtureOfGaussian(Model):\n",
    "    def __init__(self, train_x, train_y, positive_label, negative_label):\n",
    "        super().__init__(train_x, train_y)\n",
    "        self.positive_label = positive_label\n",
    "        self.negative_label = negative_label\n",
    "        \n",
    "    \n",
    "        # MLE Estimation for parameters\n",
    "        \n",
    "        y_list = train_y.values.flatten().tolist() #convert df to seq list\n",
    "\n",
    "        pi = sum([1 if y == self.positive_label else 0 for y in y_list])/len(y_list)   # pi = summation-n(yn/N)\n",
    "\n",
    "\n",
    "        #calculate mu1, mu2, s1, s2, cov, inv_cov\n",
    "        \n",
    "        positive_indices = [ind for ind, y in enumerate(y_list) if y == positive_label]\n",
    "        negative_indices = [ind for ind, y in enumerate(y_list) if y != positive_label] \n",
    "\n",
    "        n1 = len(positive_indices)\n",
    "        n2 = len(negative_indices)\n",
    "\n",
    "        N = n1 + n2\n",
    "\n",
    "        positive_X_df = train_x.iloc[positive_indices]\n",
    "        negative_X_df = train_x.iloc[negative_indices]\n",
    "\n",
    "        mu1 = positive_X_df.mean(axis=0)\n",
    "        mu2 = negative_X_df.mean(axis=0)\n",
    "\n",
    "        positive_X = positive_X_df.to_numpy()\n",
    "        negative_X = negative_X_df.to_numpy()\n",
    "\n",
    "        positive_dists = positive_X_df.to_numpy() - mu1.to_numpy()\n",
    "        negative_dists = negative_X_df.to_numpy() - mu2.to_numpy()\n",
    "\n",
    "        s1 = positive_dists.T.dot(positive_dists) / n1\n",
    "        s2 = positive_dists.T.dot(positive_dists) / n2\n",
    "\n",
    "        cov = (n1/N) * s1 + (n2/N) * s2\n",
    "        cov_inv = np.linalg.inv(cov)\n",
    "\n",
    "        w = cov_inv.dot(mu1 - mu2)\n",
    "        w_0 = -(1/2) * mu1.T.dot(cov_inv).dot(mu1) + (1/2) * mu2.T.dot(cov_inv).dot(mu2) + np.log(pi/ (1-pi))\n",
    "\n",
    "        self.pi = pi\n",
    "        self.n1 = n1\n",
    "        self.n2 = n2\n",
    "        self.mu1 = mu1\n",
    "        self.mu2 = mu2\n",
    "        self.cov = cov\n",
    "        self.w = w\n",
    "        self.w_0 = w_0 \n",
    "\n",
    "    def predict_prob(self, x):\n",
    "        logit_odds = self.w.dot(x) + self.w_0\n",
    "        prob = 1 / (1 + np.exp(-logit_odds))        \n",
    "        return prob\n",
    "    \n",
    "    def predict(self, x):\n",
    "        prob = self.predict_prob(x)\n",
    "        if prob > 0.5:\n",
    "            return self.positive_label\n",
    "        else:\n",
    "            return self.negative_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Accuracy: 0.836000\nTest Accuracy: 0.854545\nTraining Time: 0.022997617721557617\n"
     ]
    }
   ],
   "source": [
    "#train and test \n",
    "train_x, train_y = CVData.getAllXY()\n",
    "\n",
    "# Create hypothesis\n",
    "t1 = time.time()\n",
    "model = MixtureOfGaussian(train_x, train_y, POSITIVE_LABEL, NEGATIVE_LABEL)\n",
    "t2 = time.time()\n",
    "\n",
    "# Train Accuracy \n",
    "predicted_train_y = model.predict_df(train_x)\n",
    "train_accuracy = getAccuracy(train_y.values.flatten(), predicted_train_y.values.flatten())\n",
    "\n",
    "# Test Accuracy\n",
    "predicted_test_y = model.predict_df(test_X)\n",
    "test_accuracy = getAccuracy(test_Y.values.flatten(), predicted_test_y.values.flatten())\n",
    "\n",
    "\n",
    "print(\"Train Accuracy: %f\" % train_accuracy)\n",
    "print(\"Test Accuracy: %f\" % test_accuracy)\n",
    "print(\"Training Time: \" + str(t2-t1))"
   ]
  },
  {
   "source": [
    "## Logistic Regression Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(Model):\n",
    "    def __init__(self, train_X, train_y, positive_label, negative_label, lmbda=0,  max_iters=10, threshold=0.01, train_intercept=True):\n",
    "        super().__init__(train_X, train_y)\n",
    "        self.positive_label = positive_label\n",
    "        self.negative_label = negative_label\n",
    "        self.lmbda = lmbda\n",
    "        self.max_iters = max_iters\n",
    "        self.threshold = threshold\n",
    "        self.train_intercept = train_intercep\n",
    "\n",
    "        X = train_X.to_numpy()\n",
    "        y = train_y.to_numpy()\n",
    "\n",
    "        # y's as 1 & 0's\n",
    "        y = [1 if y_i == positive_label else 0 for y_i in y]\n",
    "\n",
    "        # Add column of ones\n",
    "        if train_intercept:\n",
    "            X = np.insert(X, 0, np.ones(len(X)), axis=1)\n",
    "\n",
    "        self.X = X                                  #(1000, 64) Shape\n",
    "        self.y = y                                  #(1000,1) Shape\n",
    "\n",
    "        self.w = np.zeros(X.shape[1])               #(64,) Shape\n",
    "        self._train(self.X, self.w, self.y)        \n",
    "\n",
    "        def _sigmoid(self, odds):\n",
    "            return 1 / 1 + np.exp(-odds)                \n",
    "        \n",
    "        def _gradient(self, X, w, y):\n",
    "            weighted_values = X.dot(w)\n",
    "            # probabiliteis = np.apply_along_axis(self._sigmoid, 0, weighted_values)\n",
    "            probabiliteis = self._sigmoid(weighted_values)#faster 4X than using apply_along_axis\n",
    "            return X.T.dot(probabiliteis-y) + 0.5 * self.lmbda * w.T.dot(w)\n",
    "\n",
    "        def _invProbability(self, probability):\n",
    "            return 1-probability\n",
    "\n",
    "        def _R(self, X, w):\n",
    "            weighted_values = X.dot(w)\n",
    "            probabilities = self._sigmoid(weighted_values)\n",
    "            inv_probabilities = self._invProbability(weighted_values)\n",
    "            return np.diag(np.multiply(probabiliteis, inv_probabilities))\n",
    "\n",
    "        def _Hassian(self, X, w):\n",
    "            R = self._R(X, w)\n",
    "            H = X.T.dot(R).dot(X)\n",
    "            return H + self.lmbda * np.identity(len(H))\n",
    "        \n",
    "        def _train(self, X, w, y):\n",
    "            iters = 0 \n",
    "            while (True):\n",
    "                if iters >= self.max_iters:\n",
    "                    break\n",
    "\n",
    "                gradient = self._gradient(X, w, y)\n",
    "                hassian = self._Hassian(X, w)\n",
    "                hassian_inv = np.linalg.inv(hassian)\n",
    "\n",
    "                w_new = w - hassian_inv.dot(gradient)\n",
    "\n",
    "                dist = norm(w_new - w)\n",
    "\n",
    "                w = w_new\n",
    "\n",
    "                if dist < self.threshold:\n",
    "                    break\n",
    "\n",
    "                iters += 1\n",
    "            self.w = w \n",
    "\n",
    "\n",
    "        # def _hassian(self, x, w, y):\n",
    "        #     weighted_values = x.dot(w)\n",
    "        #     term1 = self._sigmoid(weighted_values)\n",
    "        #     term2 = 1 - term1 \n",
    "        #     R = term1*term2\n",
    "        #     term3 = x.dot(x.T)\n",
    "        #     return term3.dot(R)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_x = train_x.to_numpy()\n",
    "weights_random = np.random.rand(training_data_x.shape[1])\n",
    "def sigmoid(odds):\n",
    "            return 1 / 1 + np.exp(-odds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "metadata": {},
     "execution_count": 171
    }
   ],
   "source": [
    "weighted_values = training_data_x.dot(weights_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8.01 µs ± 70.2 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sigmoid(weighted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "metadata": {},
     "execution_count": 175
    }
   ],
   "source": [
    "np.diag(np.multiply(sigmoid(weighted_values), sigmoid(weighted_values))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "47.3 µs ± 828 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.apply_along_axis(sigmoid, 0, weighted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, w, y):\n",
    "    weighted_values = x.dot(w)\n",
    "    square_part = sigmoid(weighted_values) - y \n",
    "    return square_part.T.dot(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5.19 ms ± 242 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit gradients = gradient(training_data_x, weights_random, train_y.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 64)"
      ]
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "source": [
    "gradients = gradient(training_data_x, weights_random, train_y.to_numpy())\n",
    "gradients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_along_axis(x, w, y):\n",
    "    weighted_values = x.dot(w)\n",
    "    probabilities = np.apply_along_axis(sigmoid, 0, weighted_values)\n",
    "    return (probabilities-y).T.dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7.12 ms ± 173 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit gradient_along_axis(training_data_x, weights_random, train_y.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 64)"
      ]
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "gradients_2 = gradient_along_axis(training_data_x, weights_random, train_y.to_numpy())\n",
    "gradients_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}