{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('tf2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f5c98d5dd77022b68c2195d20a02f749154dded706aacc8811bdabfa3fab50a8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Percepeptron algorithim"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "https://dafriedman97.github.io/mlbook/content/c7/concept.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libs\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Fetcher \n",
    "class dataFetcher:\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.all_data_df = pd.read_csv(file, header=None)\n",
    "        self.all_data_df = self.all_data_df.sample(frac=1).reset_index(drop=True)\n",
    "        self.training_df = self.all_data_df[int(0.15*len(self.all_data_df)) + 1:]\n",
    "        self.testing_df = self.all_data_df[:int(0.15*len(self.all_data_df))]\n",
    "\n",
    "    def _get_traning_x_df(self):\n",
    "        return self.training_df.iloc[:, :60].reset_index(drop=True)\n",
    "    \n",
    "    def _get_traning_y_df(self):\n",
    "        return self.training_df.iloc[:, 60].reset_index(drop=True)\n",
    "    \n",
    "    def _get_testing_x_df(self):\n",
    "        return self.testing_df.iloc[:, :60].reset_index(drop=True)\n",
    "    \n",
    "    def _get_testing_y_df(self):\n",
    "        return self.testing_df.iloc[:, 60].reset_index(drop=True)\n",
    "    \n",
    "    def get_all_traning(self):\n",
    "        return self._get_traning_x_df(), self._get_traning_y_df()\n",
    "    \n",
    "    def get_all_testing(self):\n",
    "        return self._get_testing_x_df(), self._get_testing_y_df()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher = dataFetcher('sonar.all-data.csv')\n",
    "traning_x_df, traning_y_df = fetcher.get_all_traning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x, y, max_iter=10):\n",
    "    X = x.to_numpy()\n",
    "    Y = y.to_numpy()\n",
    "    n_features = x.shape[1]\n",
    "    n_samples = x.shape[0]\n",
    "    w = np.random.rand(n_features)\n",
    "    for i in range(max_iter):\n",
    "        a = np.sum(x.dot(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_f = traning_x_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.rand(n_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "0    0.0195  0.0213  0.0058  0.0190  0.0319  0.0571  0.1004  0.0668  0.0691   \n",
       "1    0.0079  0.0086  0.0055  0.0250  0.0344  0.0546  0.0528  0.0958  0.1009   \n",
       "2    0.0707  0.1252  0.1447  0.1644  0.1693  0.0844  0.0715  0.0947  0.1583   \n",
       "3    0.0516  0.0944  0.0622  0.0415  0.0995  0.2431  0.1777  0.2018  0.2611   \n",
       "4    0.0392  0.0108  0.0267  0.0257  0.0410  0.0491  0.1053  0.1690  0.2105   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "171  0.0202  0.0104  0.0325  0.0239  0.0807  0.1529  0.1154  0.0608  0.1317   \n",
       "172  0.0072  0.0027  0.0089  0.0061  0.0420  0.0865  0.1182  0.0999  0.1976   \n",
       "173  0.0599  0.0474  0.0498  0.0387  0.1026  0.0773  0.0853  0.0447  0.1094   \n",
       "174  0.0272  0.0378  0.0488  0.0848  0.1127  0.1103  0.1349  0.2337  0.3113   \n",
       "175  0.0229  0.0369  0.0040  0.0375  0.0455  0.1452  0.2211  0.1188  0.0750   \n",
       "\n",
       "         9   ...      50      51      52      53      54      55      56  \\\n",
       "0    0.0242  ...  0.0261  0.0157  0.0074  0.0271  0.0203  0.0089  0.0095   \n",
       "1    0.1240  ...  0.0174  0.0176  0.0127  0.0088  0.0098  0.0019  0.0059   \n",
       "2    0.1247  ...  0.0291  0.0156  0.0197  0.0135  0.0127  0.0138  0.0133   \n",
       "3    0.1294  ...  0.0456  0.0432  0.0274  0.0152  0.0120  0.0129  0.0020   \n",
       "4    0.2471  ...  0.0089  0.0083  0.0080  0.0026  0.0079  0.0042  0.0071   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "171  0.1370  ...  0.0188  0.0127  0.0081  0.0067  0.0043  0.0065  0.0049   \n",
       "172  0.2318  ...  0.0092  0.0078  0.0071  0.0081  0.0034  0.0064  0.0037   \n",
       "173  0.0351  ...  0.0028  0.0013  0.0005  0.0227  0.0209  0.0081  0.0117   \n",
       "174  0.3997  ...  0.0146  0.0091  0.0045  0.0043  0.0043  0.0098  0.0054   \n",
       "175  0.1631  ...  0.0115  0.0064  0.0022  0.0122  0.0151  0.0056  0.0026   \n",
       "\n",
       "         57      58      59  \n",
       "0    0.0095  0.0021  0.0053  \n",
       "1    0.0058  0.0059  0.0032  \n",
       "2    0.0131  0.0154  0.0218  \n",
       "3    0.0109  0.0074  0.0078  \n",
       "4    0.0044  0.0022  0.0014  \n",
       "..      ...     ...     ...  \n",
       "171  0.0054  0.0073  0.0054  \n",
       "172  0.0036  0.0012  0.0037  \n",
       "173  0.0114  0.0112  0.0100  \n",
       "174  0.0051  0.0065  0.0103  \n",
       "175  0.0029  0.0104  0.0163  \n",
       "\n",
       "[176 rows x 60 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>55</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0195</td>\n      <td>0.0213</td>\n      <td>0.0058</td>\n      <td>0.0190</td>\n      <td>0.0319</td>\n      <td>0.0571</td>\n      <td>0.1004</td>\n      <td>0.0668</td>\n      <td>0.0691</td>\n      <td>0.0242</td>\n      <td>...</td>\n      <td>0.0261</td>\n      <td>0.0157</td>\n      <td>0.0074</td>\n      <td>0.0271</td>\n      <td>0.0203</td>\n      <td>0.0089</td>\n      <td>0.0095</td>\n      <td>0.0095</td>\n      <td>0.0021</td>\n      <td>0.0053</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0079</td>\n      <td>0.0086</td>\n      <td>0.0055</td>\n      <td>0.0250</td>\n      <td>0.0344</td>\n      <td>0.0546</td>\n      <td>0.0528</td>\n      <td>0.0958</td>\n      <td>0.1009</td>\n      <td>0.1240</td>\n      <td>...</td>\n      <td>0.0174</td>\n      <td>0.0176</td>\n      <td>0.0127</td>\n      <td>0.0088</td>\n      <td>0.0098</td>\n      <td>0.0019</td>\n      <td>0.0059</td>\n      <td>0.0058</td>\n      <td>0.0059</td>\n      <td>0.0032</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0707</td>\n      <td>0.1252</td>\n      <td>0.1447</td>\n      <td>0.1644</td>\n      <td>0.1693</td>\n      <td>0.0844</td>\n      <td>0.0715</td>\n      <td>0.0947</td>\n      <td>0.1583</td>\n      <td>0.1247</td>\n      <td>...</td>\n      <td>0.0291</td>\n      <td>0.0156</td>\n      <td>0.0197</td>\n      <td>0.0135</td>\n      <td>0.0127</td>\n      <td>0.0138</td>\n      <td>0.0133</td>\n      <td>0.0131</td>\n      <td>0.0154</td>\n      <td>0.0218</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0516</td>\n      <td>0.0944</td>\n      <td>0.0622</td>\n      <td>0.0415</td>\n      <td>0.0995</td>\n      <td>0.2431</td>\n      <td>0.1777</td>\n      <td>0.2018</td>\n      <td>0.2611</td>\n      <td>0.1294</td>\n      <td>...</td>\n      <td>0.0456</td>\n      <td>0.0432</td>\n      <td>0.0274</td>\n      <td>0.0152</td>\n      <td>0.0120</td>\n      <td>0.0129</td>\n      <td>0.0020</td>\n      <td>0.0109</td>\n      <td>0.0074</td>\n      <td>0.0078</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0392</td>\n      <td>0.0108</td>\n      <td>0.0267</td>\n      <td>0.0257</td>\n      <td>0.0410</td>\n      <td>0.0491</td>\n      <td>0.1053</td>\n      <td>0.1690</td>\n      <td>0.2105</td>\n      <td>0.2471</td>\n      <td>...</td>\n      <td>0.0089</td>\n      <td>0.0083</td>\n      <td>0.0080</td>\n      <td>0.0026</td>\n      <td>0.0079</td>\n      <td>0.0042</td>\n      <td>0.0071</td>\n      <td>0.0044</td>\n      <td>0.0022</td>\n      <td>0.0014</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>0.0202</td>\n      <td>0.0104</td>\n      <td>0.0325</td>\n      <td>0.0239</td>\n      <td>0.0807</td>\n      <td>0.1529</td>\n      <td>0.1154</td>\n      <td>0.0608</td>\n      <td>0.1317</td>\n      <td>0.1370</td>\n      <td>...</td>\n      <td>0.0188</td>\n      <td>0.0127</td>\n      <td>0.0081</td>\n      <td>0.0067</td>\n      <td>0.0043</td>\n      <td>0.0065</td>\n      <td>0.0049</td>\n      <td>0.0054</td>\n      <td>0.0073</td>\n      <td>0.0054</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>0.0072</td>\n      <td>0.0027</td>\n      <td>0.0089</td>\n      <td>0.0061</td>\n      <td>0.0420</td>\n      <td>0.0865</td>\n      <td>0.1182</td>\n      <td>0.0999</td>\n      <td>0.1976</td>\n      <td>0.2318</td>\n      <td>...</td>\n      <td>0.0092</td>\n      <td>0.0078</td>\n      <td>0.0071</td>\n      <td>0.0081</td>\n      <td>0.0034</td>\n      <td>0.0064</td>\n      <td>0.0037</td>\n      <td>0.0036</td>\n      <td>0.0012</td>\n      <td>0.0037</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>0.0599</td>\n      <td>0.0474</td>\n      <td>0.0498</td>\n      <td>0.0387</td>\n      <td>0.1026</td>\n      <td>0.0773</td>\n      <td>0.0853</td>\n      <td>0.0447</td>\n      <td>0.1094</td>\n      <td>0.0351</td>\n      <td>...</td>\n      <td>0.0028</td>\n      <td>0.0013</td>\n      <td>0.0005</td>\n      <td>0.0227</td>\n      <td>0.0209</td>\n      <td>0.0081</td>\n      <td>0.0117</td>\n      <td>0.0114</td>\n      <td>0.0112</td>\n      <td>0.0100</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>0.0272</td>\n      <td>0.0378</td>\n      <td>0.0488</td>\n      <td>0.0848</td>\n      <td>0.1127</td>\n      <td>0.1103</td>\n      <td>0.1349</td>\n      <td>0.2337</td>\n      <td>0.3113</td>\n      <td>0.3997</td>\n      <td>...</td>\n      <td>0.0146</td>\n      <td>0.0091</td>\n      <td>0.0045</td>\n      <td>0.0043</td>\n      <td>0.0043</td>\n      <td>0.0098</td>\n      <td>0.0054</td>\n      <td>0.0051</td>\n      <td>0.0065</td>\n      <td>0.0103</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>0.0229</td>\n      <td>0.0369</td>\n      <td>0.0040</td>\n      <td>0.0375</td>\n      <td>0.0455</td>\n      <td>0.1452</td>\n      <td>0.2211</td>\n      <td>0.1188</td>\n      <td>0.0750</td>\n      <td>0.1631</td>\n      <td>...</td>\n      <td>0.0115</td>\n      <td>0.0064</td>\n      <td>0.0022</td>\n      <td>0.0122</td>\n      <td>0.0151</td>\n      <td>0.0056</td>\n      <td>0.0026</td>\n      <td>0.0029</td>\n      <td>0.0104</td>\n      <td>0.0163</td>\n    </tr>\n  </tbody>\n</table>\n<p>176 rows Ã— 60 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "traning_x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(176, 60)"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "X = traning_x_df.to_numpy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(176, 61)"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "X = np.insert(X, 0, 0, axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}